--- 
title: "Data Career Opportunities Analysis"
author: "Mutian Wang, Xinyuan He, Weitao Chen, Jianing Li"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
```

# Introduction

## Background

As the development of the businesses, Data Science and Analytics are no longer just optional auxiliary accessories, they are essential business tools. As new technologies and methods make a dent in the economy, so too are they making a dent in the data science job market. Data-related jobs are being considered as emerging industry that providing lots of opportunities. 

Backed in 2012, the Harvard Business review called data scientists “the sexiest job of 21st century”. Also, according to the article “Data Scientist: A Hot Job That Pays Well” published by Indeed Hiring Lab, since 2013, the job posting of data-related job has been almost tripled while the job interests has grown more slowly. The article also mentions that the salary for data scientists varies a lot for different regions-Houston and San Francisco offer best salaries. 


## Motivation

In this project, we want to find out and verify:

1.	Whether data science industry is indeed a high-paid industry and in the stage of emerging and offering lots of positions, compared to other jobs in IT industry

2.	The regional patterns of the job posting and median pay of data-related jobs and the potential attributes that may contribute to the patterns

3.	the timeseries patterns in job opening and median pay of data related jobs and the potential attributes 

4.	the comparison between the four different data-related jobs: data scientist, data analyst, business analyst and financial analyst.

<!--chapter:end:index.Rmd-->

# Data sources

## Glassdoor
One dataset we used is gathered from the Job Market Report from Glassdoor: https://www.glassdoor.com/research/job-market-report-historical/ This dataset mainly contain the job opening number and median base pay of different type of jobs, facted by different dimensions 

One dataset we used is gathered from the Job Market Report from Glassdoor: https://www.glassdoor.com/research/job-market-report-historical/. This website has the monthly data from October, 2016 to November, 2019. 

Each monthly dataset mainly contains the national and ten cities’ job opening number and median base pay for different type of jobs, for different industry and for different company sizes. The structure of the dataset can be shown in the graph:

#####################graph needed##################

The dataset looks like:

```{r, echo=FALSE}
library(openxlsx)
df = read.xlsx("https://www.glassdoor.com/research/app/uploads/sites/2/2019/12/LPR_data-2019-03.xlsx")
head(df)
```

We encountered a major problem dealing with this data source. Glassdoor restructured and updated the datasets while we are doing data analysis part and only the datasets from June, 2017 to March, 2019 are left as same formatted and structured, so we only use the 22 months’ datasets when conducting data analysis. 


## Indeed

Since the dataset from glassdoor contain only job openings in 10 cities, and we are also interested in the frequent words in the job descriptions of different job openings, more detailed dataset are needed.

Initially, we found that Indeed.com provides a [job searching API](https://opensource.indeedeng.io/api-documentation/docs/job-search/), which allow s users to get access to data about job openings. Its results would contain information about job titles, recruiting company and location. However, applications for token of this API failed, because we are not qualified. Then, we decided to scrape data from Indeed.com instead.

[Sample search results on Web](https://www.indeed.com/jobs?q=data+sceintist&l=New+York%2C+NY)

When searching on Indeed.com, users can filter the results with key words, job titles, location or companies. In the scraper pipeline, search results on job openings with title of "Data scientist", "Data Analyst", "Financial Analyst" and "Business Analyst" around the US were included. At the first phase, scraping results are like these:

Only a preview of the job description is included, because the complete content of job description text require extra web requests, for which we concerned about its cost to Indeed.com server.

However, the analysis of the preview is not satisfying, so in the next phase, we also scraped the complete job description. The complete job description page look like this.

[Sample detail page](https://www.indeed.com/viewjob?cmp=Biz2Credit-Inc.&t=Data+Science+Manager&jk=e7c5dc3b25111e1b&sjdu=QwrRXKrqZ3CNX5W-O9jEveJgZ7DRbh_ySwPONsqRa9Y_P0u65BR9sswVPFKEuwHB-Scyziy6GodSDbmULzuMGw&tk=1drtm5c5ep7ck800&adid=320883392&pub=4a1b367933fd867b19b072952f68dceb&vjs=3)

In the search results, there exist a bunch of duplicates. That's because Indeed repeatedly presented some sponsored results.

Eventually, the scraping results contain 4 files, each for search results of one job title in the US. Duplicates have been removed. A preview of the data is as below:

```{r}
data = read.csv("../data/raw/indeed_ds.csv")
head(data)
```


## City data

### [2018 median income](https://www.statista.com/statistics/205609/median-household-income-in-the-top-20-most-populated-cities-in-the-us)

The data is available in the Statista website, originated from US Census Bureau and if connected to campus’s internet, we can download the excel file from the website for free.

The dataset is about the median household income for each city in 2018 (in U.S. dollars). There are 2 columns, city names and median household income and 50 rows in the data. The data type of columns are characters. 

There is two issues with this dataset. The city names do not match with the city names in the city_jobtitle.csv and the income column is of string type, with dollar sign($) and comma (,) in it. 

### [Population](https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk)

The data is available in the United States Census Bureau website and we can download the excel file from the website for free.

The dataset is about the estimated population for each city from 2010-2018 labeled by geographic location. There are 20 columns of some government file naming conventions and populations in each cities for 8 years, and 782 rows in the data. The location columns are character and the population columns are numeric. 

There is one issue with this dataset. The city names do not match with the city names in the city_jobtitle.csv. 

### [Violent crime](https://ucr.fbi.gov/crime-in-the-u.s/2017/crime-in-the-u.s.-2017/tables/table-8/table-8.xls/view)

The data is available in the United States Census Bureau, Population Division and we can download the excel file from it.

The dataset is about the different crime rate for each city in 2019 labeled by geographic location. There are 10 columns of city names and crime rates of different crime types in each cities and 9583 rows in the data. The location columns are character and the crime rate columns are numeric. 

There are some issues with this dataset. The city names do not match with the city names in the city_jobtitle.csv, and the year is 2017, since the statistics of 2018 is not released.

### [CPI](https://www.statista.com/statistics/245014/consumer-price-index-for-selected-us-cities)

The data is available in the United States Census Bureau website and we can download the excel file from the above link as long as we are connecting to campus Internet.

The dataset is about the CPI (consumer-price-index) levels in different us cities. There are 2 columns of city names and cpi levels, and 22 rows in the data. The location column is character and the CPI column is numeric.

There is one issue with this dataset. The city names do not match with the city names in the city_jobtitle.csv. 

<!--chapter:end:02-data.Rmd-->

# Data transformation


## Glassdoor

1.	Since the raw dataset is monthly, in order to explore the timeseries changes of median pay, we first concatenated the 22 datasets into a single data frame. 
2.	Since national data and city level data are on different levels, we separate the dataset into two main categories: national and city levels.
3.	The next step is to construct the datasets containing only the variables we are interested in, used for analysis later. Here are a few examples:
      a.	In order to focus on relationship between job title and median base pay, we subset the corresponding dataset:
```{r, echo=FALSE}
city_jobtitle = read_csv("../data/clean/glassdoor/city_jobtitle.csv")
head(city_jobtitle)
```
      b.	In order to find out the timeseries trend of change in median pay, we subset the corresponding dataset:
```{r, echo=FALSE}
na_ts_pay = read_csv("../data/clean/glassdoor/na_ts_pay.csv")
head(na_ts_pay)
```

We did the subseting for all possible dimensions and got 10 separate datasets in total, which are na_industry, na_jobtitle, na_size, na_ts_opening, na_ts_pay, city_industry, city_jobtitle, city_size and city_ts, where na stands for national; ts stands for timeseries; size stands for company size; opening stands for job opening; pay stands for median base pay. 



## Indeed

We used Indeed dataset in two parts of the analysis and conducted the data cleaning process differently based on different needs:

1. To find out the regional distribution pattern of the four data related jobs we are interested in: data analyst, data scientist, business analyst and financial analyst, we web scraped all the search result from indeed.com using the four job titles, then grouped the jobs from the same state and finally counted the number of four jobs from each state.

```{r, echo=FALSE}
df_total_state = read_csv("../data/clean/indeed/df_total_state.csv")
head(df_total_state)
```


## City data

The goal of the data cleaning is to map city statistics into 10 cities. 

To do that the median income column is supposed to be numeric but there are dollar sign and comma in this column, thus we need to remove the dollar sign and comma in this column and then convert it into numeric variables.

Firstly I used two for loops to map the city statistics into cities. However, the city names in each dataset are different, which led to some mismatch. To fix it, I assign the values manually.

There is no value of all_median_income in Atlanta in 2018. To make the Parallel Coordinates to work, I have to find this value online.

To find the patterns in different cities, I standardized the resulting dataset for using Parallel Coordinates.

We need to plot wordclouds to help us analyze. In python, I vectorized the details by 1-gram and 2-gram, removing common English stopwords and some less informative words and combining 1-gram and 2-gram into one list. The final result is in the form of word vs frequency.

<!--chapter:end:03-cleaning.Rmd-->

# Missing values


```{r, echo=FALSE}
library(tidyverse)
library(extracat)

city_job <- read_csv("../data/clean/glassdoor/city_jobtitle.csv") # no NA
nation_job <- read_csv("../data/clean/glassdoor/na_jobtitle.csv") # no NA

city_ts <- read_csv("../data/clean/glassdoor/city_ts.csv")
nation_ts <- read_csv("../data/clean/glassdoor/na_ts_pay.csv") # no NA
```

```{r, echo=FALSE}
indeed_ds <- read_csv("../data/raw/indeed_ds.csv")
indeed_da <- read_csv("../data/raw/indeed_da.csv")
indeed_fa <- read_csv("../data/raw/indeed_fa.csv")
indeed_ba <- read_csv("../data/raw/indeed_ba.csv")
```

## Glassdoor

The data set we downloaded from Glassdoor includes many missing values, but here we just analyze missing values of the data we have used. 

```{r, echo=FALSE}
city_ts %>% 
  select(-c(X1)) %>% 
  rename(Base.Pay = `Median Base Pay`, Openings = `Job Openings`) %>% 
  visna(sort = "b")
```


The data used to draw this graph is the time series of median base pay and job openings in each city. This graph is reordered both vertically and horizontally.

Column pattern: Only two variables have missing values. Median base pay has the most missing values. At least 2/3 of them are missing. As for job openings, approximately one third of them are missing. Other variables have no missing values, because they are more like "key" rather than "value".

Row pattern: Every row has at least one missing value, and there are three missing patterns: no base pay, no job openings, or both missing. It seems that median base pay and job openings cannot appear together, which is very odd. We cannot explain why this happens, but the pattern does bring many troubles to our analysis. 

Other data from Glassdoor also have missing values. We only have the base pay of each job, but not the number of job openings. Our analysis was somewhat limited due to these missing values. 


## Indeed

We scraped some job information from Indeed. These jobs are data scientists, data analysts, business analysts and financial analysts. Their missing patterns are quite similar, thus only the missing pattern of data scientists is presented here. 


```{r, echo=FALSE}
indeed_ds %>% 
  rename(post_time= posted_time) %>% 
  visna(sort = "b")
```

In this data set, each row is a job, and the column shows the information of the job. This graph is reordered both vertically and horizontally.

Column pattern: Salary has the most missing values, which is in line with our expectations. We know many companies do not disclose the salary in the preliminary stage of recruitment.

empb_id is the Indeed id of each employer, and empn_rate is the rating (scale from 0 to 5) of each employer. However, the rating is meaningless because every job of the same company will have the same rating. We don't use them in our analysis, so their absence is OK. We also don't care about Variables like detail and preview. Even though some values are missing, there's no impact. 

It's quite interesting that some company names are also missing. It's odd because company name should definitely appear in a recruitment ad. We took a look at the website of these jobs, and we found that the company name is buried in the job description part and it's hard to see.

Row pattern: There are 22 row patterns in all, and only a small number of rows contain no missing value. We cannot observe any patterns here, because the missing values seem very random.


## City data
In the all_median_income dataset, there is no all_median_income for Atlanta since the source did not release the median income for Atlanta in 2018. However, to plot the Parallel Coordinates Plot successfully, there can not be a missing value in the dataset. Thus,I have to find the the median income for Atlanta in 2017 as a substitute.


<!--chapter:end:04-missing.Rmd-->

# Results


## US labor market: Data related jobs vs. others 

The horizontal bar chart shows the jobs in the US with median base pay higher than ~60k. The base pay of each job is the average salary between Jun 2017 and Mar 2019. 



```{r, echo=FALSE}
library(zoo)
library(tidyverse)
city_job <- read_csv("../data/clean/glassdoor/city_jobtitle.csv")
nation_job <- read_csv("../data/clean/glassdoor/na_jobtitle.csv")

city_ts <- read_csv("../data/clean/glassdoor/city_ts.csv")
nation_ts <- read_csv("../data/clean/glassdoor/na_ts_pay.csv")

invisible(Sys.setlocale("LC_TIME", "C"))
city_job <- city_job %>% 
  mutate_at(vars(Value), ~str_replace(.x, "[$]", "")) %>% 
  mutate_at(vars(Value), ~str_replace(.x, ",", "")) %>%
  transform(Value = as.numeric(Value)) %>% 
  transform(Metro = as.factor(Metro)) %>% 
  transform(Month = as.Date(as.yearmon(Month)))

nation_job <- nation_job %>% 
  mutate_at(vars(Value), ~str_replace(.x, "[$]", "")) %>% 
  mutate_at(vars(Value), ~str_replace(.x, ",", "")) %>%
  transform(Value = as.numeric(Value)) %>% 
  transform(Month = as.Date(as.yearmon(Month)))

city_ts <- city_ts %>% 
  filter(!is.na(`Median Base Pay`)) %>% 
  transform(`Median Base Pay` = as.numeric(`Median Base Pay`)) %>% 
  transform(Month = as.Date(as.yearmon(Month))) %>%
  filter(Month >= as.Date("2017-06-01") & Month <= as.Date("2019-03-01")) %>%
  mutate_at(vars(Dimension), ~str_replace(.x, "Metro Median Base Pay", "Metro")) %>% 
  rename(Value = Median.Base.Pay) %>% 
  select(-c(Job.Openings))
  
nation_ts <- nation_ts %>%
  transform(Value = as.numeric(Value)) %>% 
  transform(Month = as.Date(as.yearmon(Month))) %>%
  filter(Month >= as.Date("2017-06-01") & Month <= as.Date("2019-03-01")) %>%
  mutate_at(vars(Dimension), ~str_replace(.x, "Metro Median Base Pay", "US"))
```

```{r fig.height = 8, fig.width = 8, echo=FALSE}
nation_job %>% 
  group_by(Dimension) %>%
  summarize(Pay = sum(Value)/n()) %>% 
  filter(Pay > 59000) %>% 
  ggplot() + 
  geom_col(aes(x = reorder(Dimension, Pay), y = Pay)) + 
  coord_flip() + 
  labs(x = "", y = "Median Base Pay", 
       title = "Median Base Pay (> ~60k) in the US") + 
  scale_y_continuous(labels = scales::dollar) + 
  theme_light(16) + 
  theme(plot.title = element_text(hjust = 0.5))
```

It can be seen that pharmacists, solution architects and attorneys have the highest median base pay. Besides, most of job titles in this graph contains "manager", "engineer" or "analyst". 

In fact, there are 38 jobs in the graph, and at least 8 of them are highly related to IT industry. This report mainly focuses on data scientists (DS), data analysts (DA), business analysts (BA) and financial analysts (FA), because they are the jobs that are most relevant to data science program. All of the four jobs have something to do with data, but their base pay varies. To be specific, in the Unites States, data scientists have much higher salary than DA, BA and FA. 

This pattern surprised us, because we thought these jobs are similar and they should have the same salary level. Is this just a national pattern? Can we still observe this pattern locally? 

```{r fig.height = 7, fig.width = 12, echo=FALSE}
joi = c("Business Analyst", "Data Analyst", "Data Scientist", "Financial Analyst", 
        "Java Developer", "Software Engineer", "Systems Engineer", "Web Developer")
city_job %>% 
  filter(Dimension %in% joi) %>% 
  group_by(Dimension, Metro) %>% 
  summarize(Pay = sum(Value)/n()) %>%
  ggplot() + 
  geom_col(aes(x = reorder(Dimension, Pay), y = Pay)) + 
  facet_wrap(~Metro) + 
  coord_flip() + 
  labs(x = "", y = "Median Base Pay", 
       title = "Median Base Pay in 10 US Metros") + 
  scale_y_continuous(labels = scales::dollar) + 
  theme_light(16) + 
  theme(plot.title = element_text(hjust = 0.5))
```

This graph can confirm our findings! It shows the base pay of eight IT jobs in ten metropolises. We removed other jobs which we are not interested in. 

The previous pattern is still true in this faceted bar chart. Generally speaking, in all of the ten metropoises, data scientists have the highest salary, while BA, FA and DA have the lowest. Why does this happen?

We did some research and tried to explain this pattern. In the article, [Data Analyst vs. Data Scientist](https://www.springboard.com/blog/data-analyst-vs-data-scientist), Leigh Kunis compares the differences between DS and DA. She thinks data analysts can be considered as junior data scientists. The former focuses on basic data analysis and visualization. Data scientists, however, have to do some more profound data analysis. They need to grasp machine learning skills. They also need to know how to clean data, build algorithms, design experiments and present the results. In conclusion, data scientists' work is more complicated and comprehensive.

As for financial analysts and business analysts, they need to know more domain knowledge than pure data analysts, so their salary is slightly higher than DA. Yet their analysis is still not that profound, which makes their salary lower than data scientists. In the later part, we scraped some job descriptions from Indeed and used basic NLP techniques to prove our explanation.

Based on this pattern we found, we suggest that DSI students should look for a DS job instead of a DA job.

Another pattern is that, the 8 IT jobs in this faceted bar chart are clearly divided into 4 clusters. The first cluster is the job ending with "scientist"; this cluster has the highest base pay. The second cluster is the jobs ending with "engineer"; this cluster has the second highest salary. The third cluster is the jobs ending with "developer"; its salary is the third highest. The last cluster is the jobs ending with "analyst"; its salary is the lowest among the 8 IT jobs.

This pattern is not counterintuitive at all, since there is a natural order: scientist > engineer > developer > analyst.


## Trends of data related jobs 

The line chart shows the salary of eight IT jobs in the US from Jun 2017 to Mar 2019. 

```{r, echo=FALSE}
joi = c("Business Analyst", "Data Analyst", "Data Scientist", "Financial Analyst", 
        "Java Developer", "Software Engineer", "Systems Engineer", "Web Developer")
nation_job %>% 
  filter(Dimension %in% joi) %>% 
  ggplot() + 
  geom_line(aes(x = Month, y = Value, color = fct_reorder2(Dimension, Month, Value))) + 
  geom_point(aes(x = Month, y = Value, color = fct_reorder2(Dimension, Month, Value)), size = 1) + 
  labs(x = "", y = "Median Base Pay", 
       title = "Median Base Pay in the US (2017-2019)") + 
  scale_y_continuous(labels = scales::dollar) + 
  scale_x_date(date_labels = ("%y/%m"), date_breaks = "3 months") + 
  scale_color_manual(values = c("red", "brown", "coral2", "green4", 
                                "yellow3", "magenta", "chartreuse2", "deepskyblue")) + 
  theme_light(16) + 
  theme(legend.title = element_blank(), plot.title = element_text(hjust = 0.5))
```

The graph of time series can again prove the pattern we found before. That is, from Jun 2017 to Mar 2019, the salary of DS is much higher than DA, BA and FA. 

It can also be shown that the salary of each job does not change too much since Jun 2017. Two jobs that fluctuate the most are data scientists and Java developers. The salary of software engineers and data analysts increases steadily in this period. 

In addition, the lines representing web developers and business analysts have several crosses, while the relative position of other jobs remains unchanged. 

To have a clearer view of the salary change of the data related jobs, we scaled the data and drew the line chart below. 

```{r, echo=FALSE}
joi = c("Data Analyst", "Data Scientist", "Financial Analyst", "Business Analyst")
nation_job %>% 
  filter(Dimension %in% joi) %>% 
  rbind(nation_ts) %>% 
  group_by(Dimension) %>%
  arrange(Month, .by_group = TRUE) %>%
  mutate(index = row_number()) %>% 
  mutate(tmp = if_else(index == 1, Value, as.numeric(NA))) %>% 
  fill(tmp) %>% 
  mutate(Value1 = 100*Value/tmp) %>% 
  ungroup() %>% 
  ggplot() +
  geom_line(aes(x = Month, y = Value1, color = fct_reorder2(Dimension, Month, Value1))) + 
  labs(x = "", y = "Scaled Median Base Pay",
       title = "Median Base Pay in the US (2017-2019)") + 
  scale_color_manual(values = c("deepskyblue", "black", "magenta", "chartreuse2", "red")) + 
  scale_x_date(date_labels = ("%y/%m"), date_breaks = "3 months") + 
  theme_light(16) +
  theme(legend.title = element_blank(), plot.title = element_text(hjust = 0.5))
```

In this graph, the value of the first data point is scaled to 100, and the trend of salary is magnified. In addition, the median base pay of the US is added to this graph. It can function as a reference. 

Cyclical trend: To help us recognize the cyclical trend, the tick mark labels of this graph are the first month of every quarter. However, We can hardly observe any cyclical trend, perhaps because the time span is too short. 

Secular trend: Generally speaking, every median base pay slightly increases. 2017/10 - 2017/11 seems to be a bad time when the median base pay of most jobs are the lowest. However, since 2018/08, the overall salary steps up to a higher level. 

The salary of DA fluctuates a lot, and it "greatly" drops since the beginning of 2019. The salary of DA has the best momentum, since it has the highest growth rate in 2019. The trends of BA and FA are very similar, and they are similar to the trend of US median base pay.

Since the absolute value of DA's salary is the lowest, so it's not strange that DA's salary has the highest growth rate. Similarly, the absolute value of DS's salary is very high, so it's natural that the salary drops a bit. Since BA and FA are similar, it's reasonable that there trends are similar. 

So far we have analyzed in detail the trend of median base pay in the national level. What about the city level?

```{r fig.height = 8, fig.width = 12, echo=FALSE}
joi = c("Business Analyst", "Data Analyst", "Data Scientist", "Financial Analyst")
city_job %>% 
  filter(Dimension %in% joi) %>% 
  ggplot() + 
  geom_line(aes(x = Month, y = Value, color = fct_reorder2(Dimension, Month, Value))) + 
  facet_wrap(~Metro) + 
  labs(x = "", y = "Median Base Pay", 
       title = "Median Base Pay in 10 US Metros (2017-2019)") + 
  scale_y_continuous(labels = scales::dollar) + 
  scale_x_date(date_labels = ("%y/%m"), date_breaks = "3 months") + 
  scale_color_manual(values = c("red", "magenta", "deepskyblue", "chartreuse2")) + 
  theme_light(16) + 
  theme(legend.title = element_blank(), panel.grid = element_blank(), 
        plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = -40))
```

This faceted line chart shows the change of salary in 10 metropolises from Jun 2017 to Mar 2019. Overall speaking, the change of 4 data related jobs' salary is very small. However, it seems that the salary of DS in Houston fluctuates the most. 

Obviously, the overall salary in San Francisco is the highest. This is expected because of the big techs in the Silicon Valley. 

The interest thing is that the salary of DA, BA and FA in Seattle and DC is almost the same, but the salary of DS in Seattle is much higher than that in DC. We guess seattle has more big companies, such as Amazon, Boeing, Mircosoft, etc. In fact, Seattle is also the nation's fastest growing tech hub. 

To have a clearer view of the change over time, we scaled the data and drew the following graph.

```{r fig.height = 8, fig.width = 12, echo=FALSE}
joi = c("Data Analyst", "Data Scientist", "Business Analyst", "Financial Analyst")
city_job %>% 
  select(-c(Dimension.Type, Measure, YoY)) %>%
  filter(Dimension %in% joi) %>% 
  rbind(city_ts) %>% 
  group_by(Dimension, Metro) %>%
  arrange(Month, .by_group = TRUE) %>%
  mutate(index = row_number()) %>% 
  mutate(tmp = if_else(index == 1, Value, as.numeric(NA))) %>% 
  fill(tmp) %>% 
  mutate(Value1 = 100*Value/tmp) %>% 
  ungroup() %>% 
  ggplot() + 
  geom_line(aes(x = Month, y = Value1, color = fct_reorder2(Dimension, Month, Value1))) + 
  facet_wrap(~Metro) + 
  labs(x = "", y = "Median Base Pay", 
       title = "Median Base Pay in 10 US Metros (2017-2019)") + 
  scale_color_manual(values = c("gray", "magenta", "deepskyblue", "chartreuse2", "red")) + 
  scale_x_date(date_labels = ("%y/%m"), date_breaks = "3 months") + 
  scale_y_continuous(limits = c(90, 110)) + 
  theme_light(16) + 
  theme(legend.title = element_blank(), panel.grid = element_blank(), 
        plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = -40))
```

In this graph, the value of the first data point is scaled to 100. In addition, the median base pay of each city is added to this graph. 

It can be seen that Boston, LA, NYC, SF, Chicago and Philadelphia share the similar pattern. The trend of the 4 data related jobs' base pay is close to the trend of city base pay. 

In Atlanta, the trend of BA and FA's base pay is close to the trend of city base pay. Compared to the city base pay, the salary of DS grows faster and the salary of DA grows slower. 

In Houston, the trend of DA, BA and FA's base pay is close to the trend of city base pay. The salary of DS fluctuates and decreases, compared to the city base pay. 

In Seattle, the salary of DA does not have a good growth rate. In DC, the salary of DS does not have a good growth rate.


From the above observation, we don't suggest DSI students to look for a DS job in Houston or DC, because salary there grows slowly or even negatively and its absolute value is not that high.


## Geographical patterns of data related jobs
```{r, echo=FALSE}
library(tidyverse)
df_jo_ba_state = read.csv("../data/clean/indeed/state_jo_ba.csv",na.strings = "NNN") %>% select(state, jo_ba)
df_jo_da_state = read.csv("../data/clean/indeed/state_jo_da.csv",na.strings = "NNN") %>% select(state, jo_da)
df_jo_ds_state = read.csv("../data/clean/indeed/state_jo_ds.csv",na.strings = "NNN") %>% select(state, jo_ds)
df_jo_fa_state = read.csv("../data/clean/indeed/state_jo_fa.csv",na.strings = "NNN") %>% select(state, jo_fa)

df_total_state = df_jo_ba_state %>% 
  merge(df_jo_da_state, by = "state") %>% 
  merge(df_jo_ds_state, by = "state") %>% 
  merge(df_jo_fa_state, by = "state")

df_total_state$jo_total <- rowSums(df_total_state[,c(2,3,4,5)])

df_total_state$hover <- with(df_total_state, paste(state, '<br>', "BA job openings:", jo_ba, '<br>', "DA job openings:", jo_da, '<br>', "DS job openings:", jo_ds, '<br>', "FA job openings:", jo_fa))
```

```{r, echo=FALSE}
library(plotly)
# give state boundaries a white border
l <- list(color = toRGB("white"), width = 2)

# specify some map projection/options
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('white')
)

p1 <- plot_geo(df_total_state, locationmode = 'USA-states') %>%
  add_trace(
    z = ~jo_total, text = ~hover, locations = ~state,
    color = ~jo_total, colors = 'Purples'
  ) %>%
  colorbar(title = "number of job openings") %>%
  layout(
    title = 'Data Related Job Openings Count by State<br>(Hover for breakdown by jobs)',
    geo = g,
    annotations = list(x = 1, y = -0.1, text = "BA: business analyst DA: data analyst <br> DS: data scientist FA: financial analyst", showarrow = F, xref='paper', yref='paper', 
      xanchor='middle', yanchor='auto', xshift=0, yshift=0,
      font=list(size=15, color="black"))
  )

p1
```

From the interactive map, we will be able to gain an overall sense of distribution of the job opportunities at the first glance at the map by seeing the color of each state. Also, by hovering over the state, the breakdown of number of jobs for each of the four data related jobs will show up. There are a few patterns of the regional distribution of data-related jobs:

1.	In general, east coast, west coast and Great Lakes area have more job opportunities compared to the central area. The main reason is that these areas have more technology companies and big companies, that will hire data related professionals. 
2.	Among all the states, California and New York are the two states having most data-related job opportunities. Both of the two states are having around 1,000 data-related jobs posted, which the job breakdown is very different. California is mainly hiring for Data Scientist while New York is leaning more towards Data Analyst and Business Analyst. The main reason is that there are more technical companies in California, for example companies in Silicon Valley, who will have a higher need for Data Scientists than for Business Analyst and there are more financial companies in New York area, who are mainly looking for Analyst professionals. 
3.	Almost all the states, except California, are offering more Data Analyst, Financial Analyst and Business Analyst compared to Data Scientist position, which indicates that the Data Scientist position is the rarest position among the four data-related jobs and potentially most competitive. 

In general, from this graph we can conclude that data scientist position is the rarest in most states. We would suggest people who are looking for data scientist job pay more attention to California and people who are interested in analyst related job pay more attention to east coast. 


## Best cities for data related jobs 

```{r}
library(ggplot2)
library(tidyverse)
library(psycho)
```


```{r}
city_jobtitle=read.csv("city_jobtitle.csv")

df=city_jobtitle %>%
    filter(Month=="2019-03") %>%
    filter(Dimension=="Data Scientist") %>%
   select(Metro,Value) %>%
  rename(city=Metro,median_income=Value)

df$median_income = gsub("\\$", "", Data_Analyst$median_income )
df$median_income = as.numeric(gsub("\\,", "", Data_Analyst$median_income ))
df=df %>% remove_rownames %>% column_to_rownames(var="city")
```





```{r}
df2<-read.csv("all_median_income.csv")
df2$all_median_income = as.numeric(gsub("\\,", "", df2$all_median_income))
df2=df2 %>% remove_rownames %>% column_to_rownames(var="city")
```

```{r}
for (i in rownames(df)){
   
  for (j in rownames(df2)){
    
    if(grepl(tolower(i),tolower(j))){
      df[i,"all_median_income"]=df2[j,"all_median_income"]
    }
  }
}

df["Washington DC","all_median_income"]=df2["Washington city, District of Columbia","all_median_income"]
df["Atlanta","all_median_income"]=65381  # this is 2017 stat
```


```{r}
df["population"]=NA
df_3=read.csv("population.csv")
df_3=df_3 %>% remove_rownames %>% column_to_rownames(var="city")
```

```{r}
for (i in rownames(df)){
   
  for (j in rownames(df_3)){
    
    if(grepl(tolower(i),tolower(j))){
      df[i,"population"]=df_3[j,"Population"]
    }
  }
}
```

```{r}
df["Washington DC","population"]=df_3["Washington city, District of Columbia","Population"]
```


```{r}
df_4=read_csv("violent_crime.csv")
df_4=df_4 %>% remove_rownames %>% column_to_rownames(var="city")
```

```{r}
df["violent_crime"]=NA
colnames(df_4)=c("v_c")
```


```{r}


for (i in rownames(df)){
   
  for (j in rownames(df_4)){
    
    if(grepl(tolower(i),tolower(j))){
      df[i,"violent_crime"]=df_4[j,"v_c"]
    }
  }
}
```

```{r}
df["Washington DC","violent_crime"]=df_4["Washington","v_c"]
df["New York City","violent_crime"]=df_4["New York","v_c"]
```



```{r}
df_5=read_csv("cpi.csv")
df_5=df_5 %>% remove_rownames %>% column_to_rownames(var="city")
```
```{r}
df["cpi"]=NA
colnames(df_5)=c("cpi")
for (i in rownames(df)){
   
  for (j in rownames(df_5)){
    
    if(grepl(tolower(i),tolower(j))){
      df[i,"cpi"]=df_5[j,"cpi"]
    }
  }
}
```

```{r}
df["New York City","cpi"]=df_5["New York-Newark-Jersey City, NY-NJ-PA","cpi"]
df["Washington DC","cpi"]=df_5["Washington-Arlington-Alexandria, DC-VA-MD-WV","cpi"]
```


```{r}
library(parcoords)
```


```{r}
df<-rownames_to_column(df,"city")
```


```{r}
  parcoords(
    data=df  %>% standardize() %>% column_to_rownames('city')  ,
    rownames = T 
    , brushMode = "1D-axes"
    , reorderable = T
    , queue = T
    , color = list(
      colorBy = "cpi"
      ,colorScale = "scaleOrdinal"
      ,colorScheme = "schemeCategory10"
      )
    , withD3 = TRUE
    ) 
```


(1) Briefly describe the content of the graph. What does this graph show?

This graph shows us data scientists job statistics in 10 different cities colored by each cities.

(2) State the patterns obtained from the graph.

The median income of data scientists is postively related to overall median income in each city.

The median pay of data scientists is highest in San Francisco and there are hugh gaps between the first and the second cities.

Houston, Chicago and Atlanta those three cities have relatively low data scientists median income, and low average median income, population, cpi and relatively high crime rate.

Cities with high all median income tend to have low population, vice versa

(3) Try to explain the pattern. 

One reason that San Francisco has the highest data scientist income maybe due to the high median income of all jobs in San Francisco. This may indicate that San Francisco is more developed than other cities. We can also see that working in San Francisco would feel safer than in others cities. However, this comes at some expense. The price level is San Francisco is the highest among all the cities.

(4) What can we learn from the pattern, if any? 
What can see that for data scientists job, the high pay cities is San Francisco, DC, Seattle, New York, and Los Angeles. In those cities, all cities have high price levels. Thus it may be a huge live burden. Some of the cities have high population with low crime rate, such as nyc. Some if the cities have low population but with high crime rate, such as DC. Thus the situations in cities are different and we should choose the city according to our preference.



```{r}
df["west_or_east"]=c("m","e","w","e","m","w","w","m","e","e")

  parcoords(
    data=df[order(df$median_income),][-1,] %>% standardize()%>% column_to_rownames('city') ,
    rownames = T 
    , brushMode = "1D-axes"
    , reorderable = T
    , queue = T
    , color = list(
      colorBy = "west_or_east"
      ,colorScale = "scaleOrdinal"
      ,colorScheme = "schemeCategory10"
      )
    , withD3 = TRUE
    ) 
```
(1) Briefly describe the content of the graph. What does this graph show?

This graph shows us data scientists job statistics in 10 different cities colored by geographic locations (west, middle or east).

(2) State the patterns obtained from the graph.
From this graph we can see that cities from middle part of us tend to have low cpi,high violent crime rate, low population, low data scientists median income and low overall median income.

Cities from west part of us tend to have high cpi, low violent crime rate, high data scientists median income and high overall median income.

The situations in east cities are more spreadout.

(3) Try to explain the pattern. 

One reasons that cities in the middle have low data scientists maybe that the overall median income is lower than cities in other part of US.

(4) What can we learn from the pattern, if any?
From this graph we can learn that as a data scientist, if we want to gain more, we need to avoid working in middle part of US and in Philadelphia.


```{r}
#df %>% standardize() %>% column_to_rownames('city') %>%  
#  parcoords(
#    rownames = T 
#    , brushMode = "1D-axes"
#    , reorderable = T
#    , queue = T) 
```

```{r}
#df<-rownames_to_column(df,"city")
```

```{r}
#df<-filter(df,city !="National")
```

```{r}
df["job_title"]="data_scientist"
```





```{r}
Data_Analyst=city_jobtitle %>%
    filter(Month=="2019-03") %>%
    filter(Dimension=="Data Analyst") %>%
   select(Metro,Value) %>%
  rename(city=Metro,median_income=Value)

Data_Analyst$median_income = gsub("\\$", "", Data_Analyst$median_income )
Data_Analyst$median_income = as.numeric(gsub("\\,", "", Data_Analyst$median_income ))
```

```{r}
df_Data_Analyst=merge(Data_Analyst,df[,-2][,-7])
```

```{r}
  parcoords(
    data=df_Data_Analyst  %>% standardize() %>% column_to_rownames(var="city"),
    rownames = T 
    , brushMode = "1D-axes"
    , reorderable = T
    , queue = T
    , color = list(
      colorBy = "west_or_east"
      ,colorScale = "scaleOrdinal"
      ,colorScheme = "schemeCategory10"
      )
    , withD3 = TRUE
    ) 
```
(1) Briefly describe the content of the graph. What does this graph show?

This graph shows us data analyst job statistics in 10 different cities colored by geographic locations (west, middle or east).

(2) State the patterns obtained from the graph.

From this graph we can see that cities from middle part of us tend to have low cpi,high violent crime rate, low population, low data analyst median income and low overall median income.

Cities from west part of us tend to have high cpi, low violent crime rate, high data scientists median income and high overall median income.

The situations in east cities are more spreadout.

San Francisco	has the highest data analyst median income. The cities in CA tend to have higher median income.

(3) Try to explain the pattern. 

One reasons that cities in the middle have low data analyst maybe that the overall median income is lower than cities in other part of US.

(4) What can we learn from the pattern, if any?
From this graph we can learn that as a data analyst, if we want to gain more, we need to avoid working in middle part of US and in Philadelphia.


```{r}
Business_Analyst=city_jobtitle %>%
    filter(Month=="2019-03") %>%
    filter(Dimension=="Business Analyst") %>%
   select(Metro,Value) %>%
  rename(city=Metro,median_income=Value)

Business_Analyst$median_income = gsub("\\$", "", Business_Analyst$median_income )
Business_Analyst$median_income = as.numeric(gsub("\\,", "", Business_Analyst$median_income ))
```

```{r}
df_Business_Analyst=merge(Business_Analyst,df[,-2][,-7])

parcoords(
  data=df_Business_Analyst  %>% standardize() %>% column_to_rownames(var="city"),
  rownames = T 
  , brushMode = "1D-axes"
  , reorderable = T
  , queue = T
  , color = list(
    colorBy = "west_or_east"
    ,colorScale = "scaleOrdinal"
    ,colorScheme = "schemeCategory10"
    )
  , withD3 = TRUE
  ) 
```

(1) Briefly describe the content of the graph. What does this graph show?

This graph shows us business analyst job statistics in 10 different cities colored by geographic locations (west, middle or east).

(2) State the patterns obtained from the graph.

From this graph we can see that cities from middle part of us tend to have low cpi,high violent crime rate, low population, low business analyst median income and low overall median income.

Cities from west part of us tend to have high cpi, low violent crime rate, high data scientists median income and high overall median income.

The situations in east cities are more spreadout.

San Francisco	has the highest business analyst median income. The cities in CA tend to have higher median income.

(3) Try to explain the pattern. 

One reasons that cities in the middle have low data analyst maybe that the overall median income is lower than cities in other part of US.

(4) What can we learn from the pattern, if any?
From this graph we can learn that as a business analyst, if we want to gain more, we need to avoid working in middle part of US and in Philadelphia.

```{r}
library(GGally)
ggpairs(df[,-8][,-7][,-1])
```
```{r}
ggpairs(df_Data_Analyst[,-7][,-1])
```


```{r}
ggpairs(df_Business_Analyst[,-7][,-1])
```

```{r}
df_total=df[,-7]
df_total["job_title"]="data_scientist"
df_total
```

```{r}
df_total_1=df_Business_Analyst[,-7]
df_total_1["job_title"]="Business_Analyst"
df_total_1
```


```{r}
df_total_2=df_Data_Analyst[,-7]
df_total_2["job_title"]="Data_Analyst"
df_total_2
```

```{r}
df_total_total=bind_rows(df_total, df_total_1,df_total_2)
```
```{r}
ggpairs(df_total_total, columns = 2:6, aes(color = job_title))
```

(1) Briefly describe the content of the graph. What does this graph show?

This graph shows us the median income of three jobs (data scientist, data analyst and business analyst) vs statistics of 10 different cities. We are only interesting in the first column since we want to compare the median income of three job vs statistics of cities.

(2) State the patterns obtained from the graph.

median income of those three job is postively related to cpi and over all median income and no relation to population and violent_crime.

(3) Try to explain the pattern. 

I think this makes sense since the higher the overall median income, the higher the income of our three jobs.
Also, the higher the median income, the more expensive people can afford goods, thus the cpi is higher.


(4) What can we learn from the pattern, if any?


## What are the most relevant to data related jobs? 

```{r}
library(wordcloud2)
library(tidyverse)
library(wordcloud)
```

```{r } 
df_ds=read_csv("ds_word_freq.csv")[,2:3]
set.seed(32)
wordcloud(words = df_ds$word, freq = df_ds$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.15,scale=c(2,0.5),
         # colors=rainbow)
            colors=brewer.pal(15, "Dark2"))
```

```{r}
df_da=read_csv("da_word_freq.csv")[,2:3]
set.seed(32)
wordcloud(words = df_da$word, freq = df_da$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.15,scale=c(1.8,0.5),
         # colors=rainbow)
            colors=brewer.pal(8, "Dark2"))
```
(1) Briefly describe the content of the graph. What does this graph show?
Those two wordclouds show us the job requirements of data scientists and data analysts.
(2) State the patterns obtained from the graph.
The most obvious finding is that the largest requirement for data scientist is machine learning. Data mining, analysis,visulization, deep learning and  problem solving, python and other programming language abilities are also important requirements for data scientist.

The most important requirement for data analyst is the ability to do data analysis, Also, written and verbal skills, project management, microsort office ability are also important to data analysts.

(3) Try to explain the pattern. 
Since the avarage income of data scientists is higher than data analysts, we can see that requirement for data scientists is higher than data analysts. Data scientists are required to gain more skills in programming language and cutting edge technology, such as deeping learning and machine learning. However, data analysts are more about team management and project managment.

(4) What can we learn from the pattern, if any? 
From this graph, to be good data scientists
```{r}
set.seed(32)
df_ba=read_csv("ba_word_freq.csv")[,2:3]
wordcloud(words = df_ba$word, freq = df_ba$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.15,scale=c(1.6,0.5),
         # colors=rainbow)
            colors=brewer.pal(8, "Dark2"))
```

```{r}
set.seed(32)
df_fa=read_csv("fa_word_freq.csv")[,2:3]
wordcloud(words = df_fa$word, freq = df_fa$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.15,scale=c(1.4,0.5),
         # colors=rainbow)
            colors=brewer.pal(15, "Dark2"))
```
(1) Briefly describe the content of the graph. What does this graph show?
Those two wordclouds show us the job requirements of business analysts and financial analysts.
(2) State the patterns obtained from the graph.

The most obvious finding is that the largest requirement for business analysts is project management. Having strong ability to solve clients' problems is also vital.

For financial analysts, they are required to have strong knowledge in finance and accounting and communication skills.

Microsoft office is a must-known tool for those two jobs.


(3) Try to explain the pattern. 
Since

(4) What can we learn from the pattern, if any? 
From this graph, to be good data scientists



<!--chapter:end:05-results.Rmd-->

# Interactive component

*<iframe>
<iframe>

    <head>
        <script src="https://d3js.org/d3.v5.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/2.25.6/d3-legend.js"></script>
        <style>
            svg {
                height: 500;
                width: 800;
            }

            chart-title {
                text-align: center;
                font: 300 18px / 1.2 'nyt-franklin', Arial, sans-serif;
                margin-bottom: 20px;
                color: #333;
            }

            .g-chart path {
                stroke-width: 2.5;
            }

            .player-line {
                fill: none;
                stroke-dasharray: 5, 5;
            }

            .dot {
                fill: #ffab00;
                stroke: #fff;
            }

            g.playable rect {
                fill: yellowgreen;
                opacity: 0.1;
            }

            .huge-button {
                display: block;
                /* width: fit-content; */
                padding: 0.35em 1.2em;
                border: 0.05em solid rgb(36, 17, 66);
                margin: 0 0.3em 0.3em 0;
                border-radius: 0.12em;
                box-sizing: border-box;
                text-decoration: none;
                font-family: 'Roboto', sans-serif;
                font-weight: 300;
                color: rgb(36, 17, 66);
                text-align: center;
                transition: all 0.2s;
            }

            .huge-button:hover {
                color: #FFFFFF;
                background-color: #000000;
            }

            .g-points circle {
                fill: cornflowerblue;
                stroke: cornflowerblue;
            }

            .hidden {
                display: none;
            }
        </style>
        <script>
            function groupBy(list, keyGetter) {
                const map = new Map();
                list.forEach((item) => {
                    const key = keyGetter(item);
                    const collection = map.get(key);
                    if (!collection) {
                        map.set(key, [item]);
                    } else {
                        collection.push(item);
                    }
                });
                return map;
            }
            function usePalette(key) {
                return globalPool.colorScale(key);
                // if (globalPool.palette.has(key)) {
                //     return globalPool.palette.get(key);
                // } else {
                //     pick = colorsArray.pop();
                //     if (pick) {
                //         globalPool.palette.set(key, pick);
                //         return pick;
                //     } else {
                //         throw Error("Running out of colors.")
                //     }
                // }
            }
            function getCoordinates(data) {
                return data.map(
                    elem => {
                        return {
                            k: elem.job,
                            x: new Date(elem.month),
                            y: elem.medianBasePay
                        }
                    }
                ).sort((a, b) => a.x - b.x);
            }
            function forkDataset(dataMap, hiddenKey) {
                let keys = Array.from(dataMap.keys());
                hiddenKVP = { key: hiddenKey, values: undefined }
                let kvps = keys.map(key => {
                    let arr = getCoordinates(dataMap.get(key));
                    if (key == hiddenKey) {
                        hiddenKVP.values = arr.slice(idxHiddenStart - 1);
                        lastElem = arr[idxHiddenStart - 1];
                        globalPool.playerData
                            .push([globalPool.xScale(lastElem.x), globalPool.yScale(lastElem.y)])
                        shownValues = arr.slice(0, idxHiddenStart);
                    }
                    else {
                        shownValues = arr;
                    }
                    return {
                        key: key, values: shownValues
                    };
                });
                if (!hiddenKVP.values) {
                    throw Error("Key not matched.");
                }
                globalPool.shownData = kvps;
                globalPool.hiddenData = hiddenKVP;
                return hiddenKVP;
            }
            const dataSrc = "https://raw.githubusercontent.com/lijning/career-opportunities-analysis/dev/data/clean/job-month-pay.csv";
            const chartMargin = { left: 80, bottom: 80, top: 30 };
            const idxHiddenStart = 8;
            const chartDim = { height: 400, width: 500 };
            const legendTrans = { left: chartMargin.left + chartDim.width + 50, top: chartMargin.top + 80 };
            const globalPool = {
                xScale: undefined, yScale: undefined,
                shownData: [], hiddenData: {},
                playerData: [], palette: new Map(),
                allDataPoints: [], colorScale: undefined
            };
            const colorsArray = ["#FF4136", "#2ECC40", "#FFDC00", "#001f3f", "#FF851B"]
            const jobPlayWith = "Data Scientist";
        </script>
        <script>
            function addPoints(points) {
                points.forEach(pt => { globalPool.allDataPoints.push(pt); })
                let gPt = d3.select("#diy #g-points");
                let sel = gPt.selectAll("circle");
                sel.data(globalPool.allDataPoints)
                    .enter()
                    .append("circle")
                    .merge(sel)
                    .attr('cx', d => globalPool.xScale(d.x))
                    .attr('cy', d => globalPool.yScale(d.y))
                    .transition()
                    .attr('r', 3);
            }
            function drawLines(data, flag) {
                let lineGen = d3.line()
                    .x((d, i) => { return globalPool.xScale(d.x); }) // set the x values for the line generator
                    .y((d, i) => { return globalPool.yScale(d.y); }) // set the y values for the line generator 
                    .curve(d3.curveMonotoneX);
                let lineClass;
                if (flag == 0) {
                    lineClass = "fact-line"
                } else {
                    lineClass = "answer-line"
                }
                let gPaths = d3.select("g#g-paths");

                gPaths
                    //.selectAll("g").attr("class", "g-fact-line")
                    .selectAll(`path .${lineClass}`)
                    .data(data.map(elem => elem.values))
                    .enter()
                    .append("path")
                    .attr("class", lineClass)
                    .attr("fill", "none")
                    .style('stroke', (d, i) => usePalette(d[0].k))
                    .style("opacity", 0)
                    // .attr("duration",5000)
                    .transition().duration(1000)
                    .style("opacity", 1)
                    .attr("d", lineGen);
                data.forEach(elem => { addPoints(elem.values) });
            }
            function fnClickingDraw() {
                let chart = d3.select(".g-chart");
                let coords = d3.mouse(chart.node());
                xValue = globalPool.xScale.invert(coords[0]);
                yValue = globalPool.yScale.invert(coords[1]);
                globalPool.playerData.push(coords);
                globalPool.playerData.sort((a, b) => a[0] - b[0]);
                let sel = d3.select(".g-chart").selectAll("path.player-line")
                sel.data([globalPool.playerData])
                    .enter()
                    .append("path").attr("class", "player-line")
                    .merge(sel)
                    .attr("d", d3.line())
                    .style("stroke", usePalette(jobPlayWith));
            }
        </script>
    </head>

    <body>
        <h3 class="chart-title">Guess how is data jobs' pay compared with other jobs? <br> You draw it!</h3>
        <h5>Since the beginning of 2018, the median base pay of data scientist jobs have been...</h5>
        <svg id="diy">
            <text x="20" y="10" style="font: 10px sans-serif;">Median Base Pay</text>
            <g class="g-y-axis"></g>
            <g class="g-x-axis"></g>
            <g class="g-chart">
                <g id="g-paths"></g>
                <g id="g-points"></g>
            </g>
            <g class="g-legend"></g>
        </svg>
        <div id="answer-board">
            <p id="btn-show-answer" class="huge-button">Show me the answer.</p>
            <h5 id="answer-text" class="hidden">...fluctuating and reach the peak at the end of this year. Over two
                years, it's been always the highest one.</h5>
        </div>


        <script>

            let svg = d3.select("svg#diy");
            let yAxis = svg.select(".g-y-axis")
                .attr("transform", "translate(" + chartMargin.left + ", " + chartMargin.top + ")");
            let xAxis = svg.select(".g-x-axis")
                .attr("transform",
                    `translate(${chartMargin.left},${chartMargin.top + chartDim.height})`);
            let chart = svg.select(".g-chart")
                .attr("transform", `translate(${chartMargin.left},${chartMargin.top})`);
            d3.select("svg#diy .g-legend")
                .attr('transform', 'translate(' + (legendTrans.left) +
                    ',' + (legendTrans.top) + ')')
            d3.csv(dataSrc).then(data => {
                globalPool.rawDataset = data;
                data = data.filter(row => ["Data Scientist",
                    "Software Engineer", "Product Manager"].includes(row.job));
                mapDataGrouped = groupBy(data, row => row.job);
                globalPool.mapDataGrouped = mapDataGrouped;

                let xScale = d3.scaleTime()
                    // .domain([d3.extent(data, row => new Date(row.month))])
                    .domain([new Date(2017, 5, 1), new Date(2019, 3, 1)])
                    .range([0, chartDim.width]);
                globalPool.xScale = xScale;
                let yScale = d3.scaleLinear()
                    .domain(d3.extent(data, row => +row.medianBasePay)).nice()
                    .range([chartDim.height, 0]);
                globalPool.yScale = yScale;
                xAxis.call(d3.axisBottom(xScale));
                yAxis.call(d3.axisLeft(yScale));

                let hiddenKVP = forkDataset(mapDataGrouped, jobPlayWith);
                globalPool.colorScale = d3.scaleOrdinal()
                    .domain(["Data Scientist", "Software Engineer", "Product Manager"])
                    .range(colorsArray.slice(0, 3));

                let leftBoundPlayable = xScale(hiddenKVP.values[0].x);
                let playground = chart.append("g")
                    .attr("class", "playable")
                    .attr("transform", `translate(${leftBoundPlayable},0)`);
                let clickboard = playground.append("rect")
                    .attr("width", chartDim.width - leftBoundPlayable)
                    .attr("height", chartDim.height)
                    .attr("id", "playboard");
                clickboard.on("click", fnClickingDraw);
                drawLines(globalPool.shownData, 0);
                let legendOrdinal = d3.legendColor()
                    .shape("path", d3.symbol().type(d3.symbolCircle).size(100)())
                    .shapePadding(10)
                    //use cellFilter to hide the "e" cell
                    .cellFilter(function (d) { return d.label !== "e" })
                    .scale(globalPool.colorScale);
                d3.selectAll("#diy .g-legend").call(legendOrdinal);
            });
            d3.select("#btn-show-answer").on("click", () => {
                drawLines([globalPool.hiddenData], 1);
                d3.select("#btn-show-answer").style("display", "none");
                d3.select("#answer-text").style("display", "block");
            });

        </script>
    </body>
</iframe>
<iframe/>*

<!--chapter:end:06-interactive.Rmd-->

# Conclusion

## Overview

{
[xh]
Brief overview of the whole report
}


## Limitations and future work

{
[xh]
Limitations and future directions, lessons learned.
}

<!--chapter:end:07-conclusion.Rmd-->

