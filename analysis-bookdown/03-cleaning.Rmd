# Data transformation


## Glassdoor

1.	Since the raw dataset is monthly, in order to explore the timeseries changes of median pay, we first concatenated the 22 datasets into a single data frame. 
2.	Since national data and city level data are on different levels, we separate the dataset into two main categories: national and city levels.
3.	The next step is to construct the datasets containing only the variables we are interested in, used for analysis later. Here are a few examples:
      a.	In order to focus on relationship between job title and median base pay, we subset the corresponding dataset:
```{r, echo=FALSE}
city_jobtitle = read_csv("../data/clean/glassdoor/city_jobtitle.csv")
head(city_jobtitle)
```
      b.	In order to find out the timeseries trend of change in median pay, we subset the corresponding dataset:
```{r, echo=FALSE}
na_ts_pay = read_csv("../data/clean/glassdoor/na_ts_pay.csv")
head(na_ts_pay)
```

We did the subseting for all possible dimensions and got 10 separate datasets in total, which are na_industry, na_jobtitle, na_size, na_ts_opening, na_ts_pay, city_industry, city_jobtitle, city_size and city_ts, where na stands for national; ts stands for timeseries; size stands for company size; opening stands for job opening; pay stands for median base pay. 



## Indeed

We used Indeed dataset in two parts of the analysis and conducted the data cleaning process differently based on different needs:

1. To find out the regional distribution pattern of the four data related jobs we are interested in: data analyst, data scientist, business analyst and financial analyst, we web scraped all the search result from indeed.com using the four job titles, then grouped the jobs from the same state and finally counted the number of four jobs from each state.

```{r, echo=FALSE}
df_total_state = read_csv("../data/clean/indeed/df_total_state.csv")
head(df_total_state)
```


## City data

The goal of the data cleaning is to map city statistics into 10 cities. 

To do that the median income column is supposed to be numeric but there are dollar sign and comma in this column, thus we need to remove the dollar sign and comma in this column and then convert it into numeric variables.

Firstly I used two for loops to map the city statistics into cities. However, the city names in each dataset are different, which led to some mismatch. To fix it, I assign the values manually.

There is no value of all_median_income in Atlanta in 2018. To make the Parallel Coordinates to work, I have to find this value online.

To find the patterns in different cities, I standardized the resulting dataset for using Parallel Coordinates.

We need to plot wordclouds to help us analyze. In python, I vectorized the details by 1-gram and 2-gram, removing common English stopwords and some less informative words and combining 1-gram and 2-gram into one list. The final result is in the form of word vs frequency.
