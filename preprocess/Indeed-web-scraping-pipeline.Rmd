---
title: "Indeed Data Scraping Pipeline"
author: "Jianing Li"
date: "11/30/2019"
output: html_document
---

```{r setup, include=FALSE, root.dir=TRUE}
knitr::opts_chunk$set(echo = TRUE)
if(!file.exists("data/raw")){
        root <- rprojroot::find_rstudio_root_file()
        knitr::opts_knit$set(root.dir = root)
}
```

## Instructions

Don't run all the chunks.

## Testing paths

It should be the root of this project.

```{r}
getwd()
```

## Importing functions

```{r sourcing}
source("preprocess/lib-web-scraping.R")

do.it <- function(target) {
        rt1 = search.job(
                "",
                title.words = target,
                # location = "Alsace, CA",
                job.type = JOB.TYPE$intern
        )
        path <- rt1$path %>% str_remove("=internship")
        if (file.exists(path)) {
                df = read_csv(path)
        } else {
                rt2 = search.job(
                        "",
                        title.words = target,
                        # location = "Alsace, CA",
                        job.type = JOB.TYPE$full.time
                )
                df = bind_rows(rt1$data, rt2$data) %>% unique()
                write_csv(df, path)
        }
        rt = add.job.details.on(df = df, path = path, max.size = 1500)
        # file.remove(rt1$path)
        # file.remove(rt2$path)
        return(rt)
}
```

## Do scraping

All scraped data will be saved to 'data/raw'. Following chunks searched for jobs anywhere in the states.

### 1

```{r}
df1 = do.it("data scientist")
target = "data scientist"
```

### 2

```{r}
df2 = do.it("data analyst")
```

### 3

```{r}
df3 = do.it("business analyst")
```

### 4

```{r}
df4 = do.it("financial analyst")
```
